import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Specify the LLM model
model_name = "microsoft/Phi-3-mini-4k-instruct"

# Configure for GPU usage
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True,
)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Create a pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Generation parameters
generation_args = {
    "max_new_tokens": 512,
    "return_full_text": False,
}

def chat(input):
  # Structure the prompt
  messages = [
      {"role": "user", "content": input}
  ]

  output = pipe(messages, **generation_args)
  return output[0]['generated_text']

# Example usage:
while True:
  user_input = input("You: ")
  if user_input.lower() == "exit":
    break
  response = chat(user_input)
  print("Phi-3-mini:", response
